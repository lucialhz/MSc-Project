\section{Introduction}\label{litIntroduction}

This section analyses and summarises contributing and related academic work applicable to this project.

It will be broken up into several sections: the first section ``Subject-matter Surveys'' will discuss works which describe the problems to be addressed and existing solutions to said problems, this section also included a subsection on FPGA security which makes useful reading as many embedded systems are FPGA-based. The second section ``Attacks'' contains a brief look at other physical attacks not addressed in the first section. The third section ``Solutions'' gives a deeper analysis of a handful of existing solutions, some of which directly address binding of software and hardware and some of which focus on the related subject of secure software execution and attestation. The fourth section ``Primitives'' provides a brief introduction into some of the founding principles used in many of the solutions already described and which will be heavily used in this project. Finally the conclusion will sum up the literature seen so far and describe its place in relation to this project.

\section{Subject-matter Surveys}\label{subjectMatterSurveys}

Many surveys on solutions which increase security for firmware/software in embedded systems have been completed. One such survey \cite{Shepherd2016} focusses on existing mature solutions, while others \cite{Bryant2004}, \cite{Collberg2002} and \cite{Theissing2013} provide a look at broader principles. All of these surveys also paint a picture of the attacks and threats which embedded systems face. Other surveys exist on the technologies described in this project, one such survey is \cite{Drimer2008} which focuses on FPGA security.

These subject-matter surveys will be discussed in \ref{solutionSurveys} and \ref{faultInjectionDefenceSurveys} and a deeper analysis of some of the solutions presented will be discussed in \ref{bindingSolutions}, \ref{secureExecutionSolutions} and \ref{staticAttestationSolutions}.


\subsection{Surveys on Existing Solutions for IP Protection and Secure Execution}\label{solutionSurveys}

A survey in to technologies designed to ascertain trust for embedded systems is provided in \cite{Shepherd2016}. They compare various technologies, some of which are mature and some in their infancy. Studied solutions include: Trusted Platform Module (TPM), Secure Elements(SE), hypervisors and virtualisation (e.g. Java Card and Intel's Trusted eXecution technology), Trusted Execution Environments (TEEs), Host Card Emulation (HCE) and Encryption Execution Environments (E3 - which has also been directly discussed in \cite{Lee2016}). The paper's authors set out a series of criteria which solutions are tested against, including such criteria as ``Centralised Control', where the trust technology is under the control of the issuer or the maintainer, and ``Remote Attestation'' where the trust technology provides assurance to remote verifiers that the system is running as expected. The paper goes on to describe each technology in a small amount of detail and populates a matrix of technologies vs. criteria. 

In a survey of anti-tamper technologies \cite{Bryant2004}, a series of \textit{cracking} threats and software and hardware protection mechanisms are described, many of which apply to embedded systems. Such threats include: 
\begin{itemize}
	\item Reverse engineering, achieved through a variety of methods including gaining an understanding	of software or simply \textit{code lifting} where sections of code are re-used without understanding of their functionality;
	\item Violating code integrity, where code in injected into a running program to make it carry out illegal actions outside of the desired control-flow of the program.
\end{itemize} 
Hardware solutions described include: using a trusted processor used to secure the boot of the system, using hardware to decrypt encrypted software from the hard-drive and RAM, using a hardware \textit{token} which is required to be present for the software to run.

The advantages of using hardware solutions include: using a complex CPU which is difficult to defeat while not redirecting resource from the processor used for standard operation, it is more costly to repeat attacks on hardware than it is for software (physical access is required each time) and secure hardware can also control which peripherals can be connected to the system and which software (signatures) can be allowed to run. There are some disadvantages of using hardware solutions which need to be considered, including:
\begin{itemize}
	\item Secure data traversing the secure to non-secure boundary needs to be encrypted (which creates an additional overhead for the main processor)
	\item Hardware solutions tend to be inflexible and less secure than commonly assumed
	\item Additional components can add to the cost of manufacture, which is a high priority for embedded systems design.
\end{itemize}
Software solutions described include:
\begin{itemize}
	\item Encryption wrappers, where all or just the critical portions of software are stored in a ciphertext form and dynamically decrypted. The value of this is that the attacker will not see all of the source program at the same time, however they can piece it together through snapshots or simply learn the encryption key/s. This paper does not cite any references for the subject of encryption wrappers;
	\item Code obfuscation, where the look of the code is adjusted to make it not easily readable or understandable by the attacker but performs in the same manner;
	\item Software watermarking and fingerprinting, which can be used for proof of ownership or authorship and for finding the source of leak of the software;
	\item Guarding, which is the act of adding code purely to perform anti-tamper functionality. An example of guarding is comparing checksums of running code to expected value and performing certain actions if they do not match. It is recommended that guarding is implemented automatically rather than manually as providing sufficient coverage is a complex task. It is also noted that a guard should not react immediately so as to not reveal the point in the code which triggered it.
\end{itemize}
The paper also describes a series of steps to take when using anti-tamper technology as put forward by the ``Defence Acquisition Guidebook'' created by the Defence Acquisition University \cite{DAU2011}.

A similar survey \cite{Collberg2002} covers three types of attacks: reverse engineering, software piracy and tampering which it describes as ``malicious host attacks''. To defend against such attacks the paper states three corresponding defences: code obfuscation (as well as anti-disassembly and anti-debugging measures), watermarking and tamper-proofing. The authors note that they could not find a wealth of information on tamper-proofing at the time of writing (2002) but they do draw an interesting parallel with the anti-tamper mechanisms used in computer viruses.  

\subsection{Defence against fault injection}\label{faultInjectionDefenceSurveys}

A series of high-coverage tests for security protections against fault injection attacks were run and described in \cite{Theissing2013}. It describes 17 different countermeasures, including: countermeasures protecting the data layer, combinations of data protection methods, countermeasures protecting control flow layer, combinations of control flow protection methods and combinations of data and control flow protection. To test these methods the authors produced a high number of simulated fault injections on a simulator of an ARM-Cortex-M3 processor running a benchmark application representing a bank card.

The experiments found that a combination of redundant condition checks (such as data duplication) and source and destination IDs reached the best coverage with moderate performance overhead. They also found that simple ID-based inter-block control checking were able to outperform more sophisticated (and complex) methods such as Control-Flow Checking by Software Signatures (CFCSS) as seen in \cite{Werner2016} and Assertions for Control-Flow Checking (ACFC) seen in \cite{Goloubeva2003}.

\subsection{FPGA Security}

An excellent high-coverage survey on FPGA security is provided in \cite{Drimer2008}, its contents include the background of FPGAs, attacks associated with FPGAs, defences for protecting FPGA implementations (existing at the time and ongoing research) and many more. 

\section{Attacks}

The following are some examples of analysis of threats, all of which are aimed towards disrupting the flow of software, the likes of which are the focus secure software execution solutions. 

Attacks which can be used to break instruction-level countermeasures are described in \cite{Yuce2016a}. This paper discusses various attack countermeasures and how these are circumvented. The only countermeasures addressed in this paper are algorithm-level and instruction-level (both of which are mostly redundancy-based). This paper suggests that a purely software-based countermeasure could be a futile defence.

Findings that physical faults can be injected in a non-random manner and in a low cost environment are presented in \cite{Kelly2017}, this contradicts assumptions made in many of the examined solutions that physical attacks are too costly. It finds that instruction-skipping attacks create a vulnerability to skipped-instruction errors (which, in my opinion, drives the motivation behind control-flow monitoring right down to the intra-block level).

Further details on side-channel attacks, as well as a brief description of the security concerns associated with FPGAs are provided in \cite{GebotysCatherineH2010Sied}.

\section{Solutions} \label{solutionsDescriptions}

A myriad of creative technical solutions have been put forward which address the problems already discussed. They can be placed in to one of two categories - binding hardware \ref{bindingSolutions} (\cite{Lee2016}, \cite{Schaller2014}, \cite{Gora2010} and \cite{Simpson2006}) and software or secure software execution \ref{secureExecutionSolutions} (\cite{Werner2016}, \cite{Wang2016}, \cite{Abera2016} and \cite{Arora2006}). Hybrids of the two approaches are presented in \cite{Kleber2015} and \cite{Kohnhauser2015}. Attestation is another important aspect and can be split into static (\cite{For},\cite{Li2011},\cite{Seshadri2007},\cite{Seshadri2004},\cite{AbuHmed2009}) and dynamic (\cite{Kil2009}) attestation, we have also included a discussion on attacks against various attestation methods\cite{Castelluccia2009} and a short conclusion discussing attestation.

\subsection{Binding Hardware and Software}\label{bindingSolutions}

Hardware software binding is a technique where hardware and software a co-designed in such a way that software needs to be tailored to run on an individual instance of hardware. The same principle works the other way in that a individual piece of hardware will not execute software unless is it specifically tailored to it.

The first piece of work we consider is \cite{Lee2016}. The problem the paper aims to address is device counterfeiting. An example of the requirement for binding of hardware and software is for Graphics Processing Units (GPUs), where GPUs are fabricated and then tested on their operating performance and subsequently graded. Once graded, the GPUs are loaded with firmware which controls their voltage and clockspeed. The paper states that firmware aimed towards the superior graded GPUs could be installed on lesser graded GPUs which would then be sold on as superior GPUs.

The attacker described in \cite{Lee2016} is one which has several special attributes: they have physical access the the device, access to the device storage where they can read and copy the entire contents of memory, they are able to use hardware which has been built to the exact specifications as the original hardware and they can ``read and copy any data which is loaded onto any of the buses which make up the embedded system''. The attacker's aim is to either create a counterfeit platform which performs and functions in the same manner as the original or to install software retrieved from the legitimate product onto different (counterfeit) hardware.

The method presented in \cite{Lee2016} uses a function applied to either previous contents of memory or a randomly generated number to produce a mask which is applied to the program instructions residing in memory. The intention is that the CPU unmasks the contents as part of the execution process prior to actually carrying out the operation. The paper discusses the options for the mask-creating function, suggesting the use of hash-functions, block ciphers or PUFs before finally selecting PUFs due to their intrinsic nature. The act of masking the software has been undecided in this paper, which suggests that either the software is masked prior to loading or is masked during the loading process. The paper's author describes the provisioning process in a further paper \cite{Lee2017}.


The second piece of work we consider is \cite{Schaller2014} where the goal is to ``protect against intellectual property (IP) extraction or modification on embedded devices without dedicated security mechanisms''. In this paper the attacker is aiming to extract IP (in the form of software or secrets) stored on the device. The attacker may use this information in any of the following ways: they may implement the extracted information on counterfeit devices, they may modify the software or data to remove licensing restrictions or unlock premium features, they may downgrade to earlier firmware versions  in order exploit previous vulnerabilities, they may wish to alter firmware to capture valuable data such as password, change output data such as readings on smart meters or reveal secrets such as cryptographic keys.

\ifnotesincluded
\furtherwork{although how does this help with this? I suppose the earlier firmware would have to have been taken from the same device.}
\fi

In \cite{Schaller2014} the attacker is assumed to have physical access to the device, can read the contents of external memory and can inspect and modify on-chip memory values. The assumed limitations placed on the attacker by the paper are that the attacker cannot change the code of the boot-loader as it is stored in a masked read-only memory (ROM), they cannot replace the ROM chip with one with a boot-loader under the attacker's control as the ROM chip would be heavily integrated on a system-on-a-chip (SoC) so would require skill levels outside of those expected of the attacker and finally the attacker cannot read the start-up values of the on-chip SRAM during start-up which are protected by the boot-loader and are erased once read by the boot-loader.

\ifnotesincluded
\furtherwork{How are the start-up values on the chip protected?}
\fi

The method described in \cite{Schaller2014} heavily utilises PUFs created using the SRAM start-up values to derive a key used to decrypt the firmware. The firmware is decrypted by the the boot-loader before being loaded and executed. The system was implemented on a SoC platform using a two-stage bootloader (u-boot). The paper's authors provide an extensive review of SRAM PUFs for ARM Cortex-M and Pandaboard's IMAP and includes a description of Fuzzy Extractor design used by the solution.

\ifnotesincluded
\furtherwork{are the where are the plaintext instructions now stored?}
\fi


The third piece of work \cite{Kleber2015} has not been published in a well-established journal however the authors have been invited to present their findings in \cite{Kleber2015a}. Here the problem of injection of malicious code is also addressed, as well as prevention of code reverse-engineering. This paper uses secure execution to bind hardware to software.

The attacker identified in \cite{Kleber2015} has physical access to the processor and peripheral connections and that they can read out contents of memory or registers. They are also assumed to be able to place arbitrary data into the main memory of the processor (either locally or remotely). Attacks comprising denial-of-service (DoS)achieved by, for example, injection of random invalid instructions and hardware side-channel attacks have not been addressed.

The method described has been labelled ``Secure Execution PUF-based Processor'' or SEPP. The operating principle of SEPP is the encryption of basic blocks (which have exactly one entry point and one exit point) which make up programs. The blocks are encrypted using a symmetric cipher in CTR mode with the parameters set in relation to instruction location within a block and the block's location within memory. The key used for this encryption is set by the user. SEPP utilises a ring-operator (RO) PUF to create a new key used to encrypt the users key. The decryption module is included in the instruction fetch stage of the processor's pipeline and makes use of first-in, first-out (FIFO) buffer to store encryption pads before they are needed by the processor (therefore making use of spare time provided by instructions which take more than one processor cycle). This system also implements u-boot as a bootloader which has been modified to provide the functions of the security kernel. It appears that due to the nature of this method (the device tailoring the software to itself), it does not prevent malices uploading of a new program to device which the device then processes.

\ifnotesincluded
\furtherwork{Where us Ku (the user key) stored? As it is used to decrypt it is surely readable by the attacker? If so the programmer could be extracted and decrypted.}
\fi


The fourth method identified in \cite{Kohnhauser2015} had identified illegitimate reproduction as a problem that requires a solution. It also identifies modification of software to bypass the need for purchasing a license for particular features as another attack scenario. Here the attacker has the ability to read and modify the content of external memory such as flash memory or RAM, they can also do the same with internal memory including software with hard-coded secrets and cryptographic keys. The method consists of four basic mechanisms: two check functions and two response functions. The first check function hashes the native program code and compares this to the current running code. The second check function uses a SRAM-derived PUF to measure the authenticity of the device. If these functions indicate that either the software is not in its intended state or is running on the incorrect device the first response function adjusts the flow of the program to move in a random manner and the second response corrupts the program's execution stack. Both response functions are designed to cause a malfunction in the program. One has to question the safety of having a program jump to a random block.


The fifth method described in \cite{Gora2010} is developed to protect against IP theft or reverse engineering. This paper does not describe the attacker but makes some assumptions that they will not be able to access the PUF-based key used for encryption as it is internal to the FPGA. This method uses an obfuscated secure ROM to start the boot process, checking and running the integrity kernel which decrypts and runs the software using the PUF-based key. In my opinion, the problem with this solution is the reliance on an obfuscated ROM. This is because once there is an understanding of a ROM it could be possible for malicious software to be written in a manner that is accepted by the boot program in ROM.

\ifnotesincluded
\furtherwork{So is is the obfuscated ROM the same on each chip and will it be able to be understood in order to create malicious integrity and security kernels or just the software? Also once decrypted where are the plain text instructions stored?}
\fi

An honourable mention should be made for \cite{Simpson2006} which was published in 2006 and led the way in using PUFs to secure software and also clearly describes the enrolment process with defined message exchanges.

\ifnotesincluded
\furtherwork{Actually why have I ranked this one so low? If it's just because of its age I should re-review}.
\fi


\subsection{Secure execution}\label{secureExecutionSolutions}

Secure software execution (or control-flow security/integrity) has the goal of preventing the desired results of attacks against program control flow, which aim to use physical attacks to make the execution of running programs jump to blocks which should not be running at that particular time (e.g. an administration function).

The first solution \cite{Werner2016} raises the point that most research on fault-attacks has been aimed towards cryptographic functions which result in gaining knowledge of the secret key. This paper takes this further by considering the modification of games consoles to make them skip the function which checks the validity of loaded software. The paper focuses on securing against fault-attacks.

The method mainly utilises control-flow integrity to solve the stated problem. The basic principle behind control-flow integrity is the understanding of the basic blocks of a making up a program. The blocks will flow into one another control-flow instructions. This flow can be described as the control-flow graph (CFG) and a correctly functioning program will abide by this graph. The signature of the program flow is created and compared against the expected value according to the CFG. The solution provides assistance to C programmers in that it automatically inserts signature updates, although programmers can also insert them manually for critical sections of the program. This functionality has been provided via the editing of LLVM compiler. If using assembly code the programmer must manually insert signature updates whenever branches, loops and function calls are encountered. The control flow signatures are calculated through a  recursive disassembling approach. In order to check the running  program's integrity a ``derived signature'' is calculated on the running code's path, this can then be compared to the corresponding derived signature of the intended route of the CFG. Important principles introduced in this paper (along the same lines as CFG) are generalized path signature analysis (GPSA) and continuous-signature monitoring (CSM). This paper does require modifications to be made to Cortex-M3 architecture.

\ifnotesincluded
\furtherwork{Should I directly address GPSA and CSM?} 

\furtherwork{How hard is this/ how is this done? Perhaps need more info on processor architecture}

\furtherwork{Do I include this? ``In order to check the running  program's integrity a ``derived signature'' is calculated on the running code's path, this can then be compared to the corresponding derived signature of the intended route of the CFG''?} 
\fi

The second paper \cite{Wang2016} (ConFirm) states that ``given the critical role of firmware, implementation of effective security controls against firmware malicious actions is essential'', having read the various prior examples seen we can safely agree with this. 

The attacker is assumed to have the ability to inject and execute malicious code, or call existing functions not abiding by the control-flow graph. These attacks are assumed to be possible either on-line or off-line (which would require a device reboot after uploading of malicious firmware image) and depending on the design of the device the firmware alterations could be achieved locally or remotely.

The method employed revolves around making use of hardware performance counters (HPCs) which count various types of events. The HPCs are utilized in conjunction with a bootloader (which sets checkpoints, initialises an HPC handler and contains a database of valid HPC-based signatures). While the program is run, HPC values are checked once checkpoints are reached (checkpoints are placed at the beginning and end of each basic block, as well as one randomly inserted between). The checkpoints actually redirect the control flow to the ConFirm core module to compare the HPC value with those stored in the database containing valid values. If the check fails ConFirm will report a deviation, which could be used to run a fault sequence, such as ``rebooting the system, generating an alarm and disabling the device''.

\ifnotesincluded
\furtherwork{If the database is stored in RAM how is it updated when new FW is released?}.
\fi

The third paper \cite{Abera2016} approaches secure execution from a slightly different direction: remote attestation. It aims to provide remote attestation of an application's control flow path during operation. The paper excludes physical attacks and instead focusses on execution path attacks, they assume that the subject device features data execution prevention (DEP) such as read $\oplus$ write and a secure trust anchor that provides an isolated measurement engine and can generate the fresh authenticated attestation report.

The method builds a hash of the path taken from node to node which is then reported to the verifier. Loops are dealt with in a novel manner to work around the issue posed due to the infinite hash available when the number of iterations of a loop is set dynamically.

\ifnotesincluded
\furtherwork{Read more into this}
\furtherwork{Could this be a good project basis?}
\fi

The fourth paper \cite{Arora2006} lists various attacks, ranging from buffer overflow attacks to physical attacks. Their method measures inter-procedural control flow, intra-procedural control flow and instruction stream integrity. Control flow monitoring is provided by an additional hardware element which tracks instruction addresses and compares them to known acceptable values stored in lookup tables. Instruction steam integrity monitoring utilises the lookup tables in addition to corresponding hash values of the basic clock. If a violation is discovered it is reported to the processor which should then terminate execution of the current program. Details of the violation are included in the report to the processor to enable a finer-grained view of the violation.

\subsection{Attestation of code integrity} \label{staticAttestationSolutions}

\subsubsection{SMART: Secure and Minimal Architecture for (Establishing a Dynamic) Root of Trust}
SMART \cite{For} provides a simple method of attesting the state of a user application through slight hardware alterations. As an input from the verifier it takes a nonce, start memory address and end memory address (as well as additional parameters for an optional start position of user application). The prover replies with a SHA-1 HMAC of the requested code space.

Important capabilities required by SMART are:
\begin{itemize}
	\item Secure HMAC key storage - this has not been directly addressed here however options are presented such as the key being hard-coded at production time (and never changed again) or by implementing a secure means of modifying by an authorized party, but not reading (as in the key can only be read by SMART). The second option was deferred to being future work.
	\item Secure HMAC key access - only SMART code on ROM can access the HMAC key. This is provided by registers only allowing access to the HMAC key while the program counter is located within SMART application space.
	\item It is not possible to enter or exit SMART instructions other than in the beginning or end. For example, if the program counter (\verb|pc|) is outside of SMART its previous location must also be outside of SMART (or the last instruction), and if the \verb|pc| is inside SMART its previous location must be at the start or also within SMART.
	\item Smart code is not editable, it is stored on ROM.
\end{itemize}

SMART was implemented on low-end microcontroller units (MCU)s such as MSP430 or AVR. After implementation it was realised that only the memory access controller needed to be modified, therefor making the solution possibly compatible with “black box” processors such as low-end ARM cores.

To critique the method - it uses SHA1 which is now outdated. It also could succumb to cold-boot attacks but the authors state that due to the typical MCU design where the processor and memory are a single package meaning that memory could not be accessed directly.

\subsubsection{VIPER: Verifying the Integrity of PERipherals’ Firmware}

VIPER \cite{Li2011} is a software only solution designed to provide attestation for peripherals’ firmware, where the host CPU queries peripherals. It uses combinations of checksums, hashes and time-frame based checks to ensure peripheral firmware is correct.

\subsubsection{Pioneer: Verifying Code Integrity and Enforcing Untampered Code Execution on Legacy Systems}

Pioneer \cite{Seshadri2007} is a software-only solution which works in a similar way to VIPER but is designed to attesting legacy systems. A checksum is calculated throughout the execution of the verifying (testing mechanism) code, the resulting execution time is compared to an expected execution time known to the verifier. This proves that the testing mechanism is correct. The testing mechanism then builds a hash of the target code.

\subsubsection{SWATT: SoftWare-based ATTestation for Embedded Devices}

SWATT \cite{Seshadri2004} is a software-based solution which uses a random number generator to generate random memory addresses which are attested - this means an attacker cannot predict which regions they need to leave intact. They also use time measurement to ensure an attacker is not manipulating the results. This author questions whether that time-based checking is a reliable component, especially when attestation is taking place remotely, for example, over a network.
\ifnotesincluded
\furtherwork{Do I care?} 
\fi
This paper also references solutions which use secure coprocessors which are used during system initialisation to bootstrap trust, examples of these are TCG (Trusted Computing Group, formerly known as TCPA) and Next-Generation Secure Computing Base (NGSCB, formerly Palladium) which may be of interest at a later point as we will be able to see if they produce a useful output after startup has occurred.

\subsubsection{Software-Based Remote Code Attestation in Wireless Sensor Network}

AbuHMed at al \cite{AbuHmed2009} (again software-based) focusses on filling empty memory with predictable contents (such as ciphertext generated by the verifier) in an attempt to prevent an attacker from utilizing free space for malicious instructions.

\subsection{Dynamic attestation of code integrity} \label{dynamicAttestationSolutions}

\subsubsection{Remote Attestation to Dynamic System Properties: Towards Providing Complete System Integrity Evidence}

Kil et al \cite{Kil2009} aim to solve a different problem of attestation - that of dynamic attestation. It uses various different components to measure run-time statistics such as data invariants. They state that they aim for full coverage of the source code rather than all execution paths, the problem of achieving coverage is described as the equivalent to solving the halting problem. By being heavily interwoven with the Linux kernel they track all system calls. It heavily relies on the TPM to do the heavy lifting of signing attestation reports and isolating records of violations. This solution utilises a Merkel hash tree for use in authenticating input files for the proc file.

\subsection{Attacks on software-based static attestation} \label{attestationSolutionsAttacks}

\subsubsection{On the Difficulty of Software-Based Attestation of Embedded Devices}

This paper \cite{Castelluccia2009} that software-based attestation is a problematic solution. They attack SWATT \cite{Seshadri2004} by adding a hook into the attestation code which swaps out malicious code to data memory, as a result they recommend covering all memory in attestation processes (which they note would not be easy in embedded systems). They find that the time overhead relied upon by SWATT \cite{Seshadri2004} is imprecise and that their attacks fit within the tolerance. The paper also attacks solutions which fill empty memory with noise, as they found they could compress legitimate applications to make room for malicious ones.

\subsection{Conclusions on attestation} \label{attestationConclusion}

To provide attestation one usually requires a nonce and an area of memory to attest. The problem of offline attestation (audit) is that a previous good report could be cloned. To overcome this we should consider sequence numbers - where they need to be unique each time the process occurs, these would be stored alongside the HMAC prior to being encrypted. Another option could be using a time-stamp, however the time value would have to be strongly protected and secure in order to prevent an attacker from changing the time to the desired time when the attack is planned in order to create a ``good'' report. Chaining reports together may provide protection against this.

It seems that a large amount of the focus of the software-only mechanisms focus on proving the integrity of the verification software, which is a legitimate concern. Secure operating conditions or self attestation should  be able to provide for this.

A key learning from the solutions and the attack described is the importance of attesting all parts of memory.

\section{Primitives}

\subsection{Control-flow Graphs}

The use of control flow checking for accidental program flow changes is considered in \cite{Goloubeva2003}, but still presents the basic principle. It describes basic blocks, control-flow between blocks, how these make up program graphs and finally how these graphs can be used to indicate control-flow error. The paper presents two existing error detection methods but finds faults in both so presents a novel solution addressing the previous solutions' shortcomings.

Chapter 9 of \cite{AhoAlfredV.2014C:pt} contains a wealth of information on data-flow and will provide a good basis for understanding both the essence of data(control)-flow and how compilers use them for code optimisation. This chapter should be referenced in order to gain a meaningful understanding of control-flow.


\subsection{Physical Unclonable Functions (PUFs)}\label{PUFExplaination}

A huge amount of prior research has been undertaken into PUFs and their application towards security in embedded systems. One very good overview is the PhD thesis \cite{Maes2012}, which provides a thorough examination into most aspects of PUFs including the types, analysis of each type in terms of uniqueness and reproducibility and uses including entity-authentication and key generation.

Much of the literature (\cite{Lee2016}, \cite{Schaller2014}, \cite{Kleber2015}, \cite{Kohnhauser2015}, \cite{Gora2010} and \cite{Simpson2006}) already described explain the use of PUFs and their reasoning behind their choice in PUFs.

\section{Conclusion}

In this literature review we have seen the reasons why the security of firmware of embedded devices is an important matter which needs to be addressed. We then saw reviews of existing solutions and introductions to primitives used. After an introduction to potential physical attacks we then provided an in depth review of solutions which enabled the binding of hardware and software and solutions which provide secure software and attestation of code. Finally we looked at two primitives which were prominent in many of the solutions examined.
